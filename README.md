# Deep Learning Parallelization: CNN Training on CIFAR-10

A comprehensive implementation comparing serial and parallel training strategies for a Convolutional Neural Network on the CIFAR-10 dataset.

## ğŸ“‹ Project Overview

This project implements and evaluates:
- **Serial Baseline**: Single-threaded NumPy CNN implementation
- **MPI Parallel**: Data-parallel training using MPI (mpi4py)
- **Hybrid MPI+OpenMP**: Combined MPI data parallelism with OpenMP thread parallelism (via Numba)

## ğŸ—ï¸ Architecture

```
Input (32Ã—32Ã—3)
    â†“
Conv2D(32 filters, 3Ã—3) + ReLU â†’ MaxPool(2Ã—2)
    â†“
Conv2D(64 filters, 3Ã—3) + ReLU â†’ MaxPool(2Ã—2)
    â†“
Flatten â†’ Dense(256) + ReLU â†’ Dense(10) + Softmax
    â†“
Output (10 classes)
```

## ğŸ“¦ Requirements

```bash
# Core dependencies
pip install numpy

# For MPI parallelism
pip install mpi4py

# For hybrid OpenMP parallelism
pip install numba
```

**System Requirements:**
- Linux OS (tested on Ubuntu)
- MPI implementation (OpenMPI recommended): `sudo apt install openmpi-bin libopenmpi-dev`
- Python 3.8+

## ğŸš€ Quick Start

### 1. Clone and Setup
```bash
cd /home/tensu-hiwi/Documents/Projects/home-take-programming
pip install numpy mpi4py numba
```

### 2. Run Serial Baseline
```bash
python serial_cnn.py --epochs 10 --batch-size 32
```

### 3. Run MPI Parallel (4 processes)
```bash
mpirun -np 4 python parallel_cnn_mpi.py --epochs 10 --batch-size 32
```

### 4. Run Hybrid MPI+OpenMP
```bash
export OMP_NUM_THREADS=4
mpirun -np 2 python parallel_cnn_hybrid.py --epochs 10 --batch-size 32
```

### 5. Run Live Demo (for presentations)
```bash
python demo.py
```

## ğŸ“Š Running Full Experiments

```bash
# Run all experiments (serial + MPI + hybrid)
python experiments/run_experiments.py --epochs 5 --subset 5000

# Analyze results
python experiments/analyze_results.py --results-dir ./results
```

## ğŸ“ Project Structure

```
home-take-programming/
â”œâ”€â”€ serial_cnn.py              # Serial baseline implementation
â”œâ”€â”€ parallel_cnn_mpi.py        # MPI data-parallel implementation
â”œâ”€â”€ parallel_cnn_hybrid.py     # Hybrid MPI+OpenMP implementation
â”œâ”€â”€ demo.py                    # Live demonstration script
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ layers.py              # CNN layer implementations
â”‚   â”œâ”€â”€ optimizers.py          # SGD and Adam optimizers
â”‚   â”œâ”€â”€ data_loader.py         # CIFAR-10 data loading
â”‚   â””â”€â”€ metrics.py             # Timing and metrics utilities
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_experiments.py     # Automated experiment runner
â”‚   â””â”€â”€ analyze_results.py     # Results analysis and plotting
â”œâ”€â”€ report/
â”‚   â”œâ”€â”€ report.md              # Technical report (Markdown)
â”‚   â””â”€â”€ report.tex             # Technical report (LaTeX source)
â””â”€â”€ README.md                  # This file
```

## âš™ï¸ Command Line Options

All training scripts support:
- `--epochs N`: Number of training epochs (default: 10)
- `--batch-size N`: Batch size (default: 32)
- `--lr F`: Learning rate (default: 0.01)
- `--momentum F`: SGD momentum (default: 0.9)
- `--data-dir PATH`: CIFAR-10 download directory (default: ./data)
- `--save-metrics PATH`: Path to save metrics (default: results/*.npz)
- `--subset N`: Use subset of data for quick testing

## ğŸ”¬ Parallelization Strategies

### Data Parallelism (MPI)
- Dataset is partitioned across MPI processes
- Each process computes local gradients
- Gradients synchronized via `MPI_Allreduce`
- All processes maintain identical model weights

### Hybrid MPI+OpenMP
- MPI: Data parallelism across nodes/processes
- OpenMP: Thread-level parallelism for convolutions
- Numba `prange` used for OpenMP-style loops
- Combines inter-process and intra-process parallelism

## ğŸ“ˆ Expected Results

| Configuration | Speedup | Efficiency |
|---------------|---------|------------|
| Serial (1 core) | 1.0x | 100% |
| MPI (2 procs) | ~1.9x | ~95% |
| MPI (4 procs) | ~3.5x | ~88% |
| Hybrid (2PÃ—4T) | ~5.5x | ~69% |

*Results may vary based on hardware and data size.*

## ğŸ“ Technical Report

See [report/report.tex](report/report.tex) for the professional LaTeX source or [report/report.md](report/report.md) for the Markdown version.
The report covers:
- Model architecture and design
- Parallelization approach
- Experimental methodology
- Performance analysis
- Discussion and conclusions

## ğŸ“ Author

**Tensae Aschalew**
ID: GSR/3976/17
Deep Learning Parallelization Project - Take-Home Assignment

## ğŸ“œ License

Educational use only.
